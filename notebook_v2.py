# -*- coding: utf-8 -*-
"""notebook_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RHB2IjM5Jc718RqmWG75dD-1sujYZIBB
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import os

gpus=tf.config.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu,True)

from google.colab import drive
drive.mount('/content/gdrive')

!ls -lha kaggle.json
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

!unzip imdb-dataset-of-50k-movie-reviews.zip

df=pd.read_csv('IMDB Dataset.csv')
df.head()

df.iloc[0]['review']

df.info()

df.isnull().sum()

df.describe()

import seaborn as sns
from matplotlib import pyplot as plt

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re,string

nltk.download('stopwords')

x=df['review']
y=df['sentiment']
stemmer=PorterStemmer()
stopwords.words('english')[:10]

@tf.keras.utils.register_keras_serializable()
def custom_standardize(input_data):
    text=tf.strings.lower(input_data)
    text=tf.strings.regex_replace(text,"<br />"," ")
    text=tf.strings.regex_replace(text,f"[{re.escape(string.punctuation)}]","")
    for word in stopwords.words('english'):
        text=tf.strings.regex_replace(text,f' {word} ',"")
    for word in stopwords.words('english'):
        text=tf.strings.regex_replace(text,f' {word} ',f" {stemmer.stem(word)} ")
    return text

from tensorflow.keras.layers import TextVectorization

max_tokens=30000
output_seq_len=500
embedding_dim=128
batch_size=128
time_steps=32

vectorize_layer=TextVectorization(
    standardize=custom_standardize,
    max_tokens=max_tokens,
    output_mode='int',
    output_sequence_length=output_seq_len
)
vectorize_layer.adapt(x)

x=vectorize_layer(x.values)
y=y.map(lambda x:1 if x=='positive' else 0)
x.shape,y.shape

data=tf.data.Dataset.from_tensor_slices((x,y))
data=data.cache()
data=data.shuffle(buffer_size=10000)
data=data.batch(batch_size)
data=data.prefetch(tf.data.AUTOTUNE)
data_iterator=data.as_numpy_iterator()

train_data=data.take(int(.8*len(data)))
val_data=data.skip(int(.8*len(data))).take(int(.2*len(data)+1))
print(f'Train size: {len(train_data)},\nValidation size: {len(val_data)},\nTotal size: {len(data)}')

data_iterator.next()

from tensorflow.keras import Model,Input
from tensorflow.keras.layers import Embedding,Dense,LSTM,Bidirectional,Dropout

def build_model():
    inputs=Input(shape=(output_seq_len,))
    x=Embedding(max_tokens+1,embedding_dim)(inputs)
    x=Dropout(.2)(x)
    x=Bidirectional(
        LSTM(time_steps,return_sequences=True,dropout=0.2)
    )(x)
    x=Bidirectional(
        LSTM(time_steps,dropout=.2)
    )(x)
    x=Dense(1,activation='sigmoid')(x)

    model=Model(inputs=inputs,outputs=x)
    return model

model=build_model()
model.summary()

tf.keras.utils.plot_model(model,show_shapes=True,dpi=96,show_layer_activations=True)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy']
)

history=model.fit(
    train_data,
    epochs=10,
    validation_data=val_data,
    workers=-1,
    callbacks=[
        tf.keras.callbacks.TensorBoard(log_dir='logs',write_images=True,write_steps_per_second=True),
        tf.keras.callbacks.ModelCheckpoint(filepath='ckpt',verbose=1,save_best_only=True)
    ]
)

df=pd.DataFrame(history.history)
df.to_csv('model_metrics.csv')

df=pd.read_csv('model_metrics.csv')
plt.style.use('fivethirtyeight')
fig,ax=plt.subplots(nrows=1,ncols=2)
fig.set_figwidth(15)
fig.set_figheight(5)
ax[0].plot(df['loss'],label='loss')
ax[0].plot(df['val_loss'],label='val loss')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Loss')
ax[0].set_title('Loss trend')

ax[1].plot(df['accuracy']*100,label='accuracy')
ax[1].plot(df['val_accuracy']*100,label='val accuracy')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Accuracy')
ax[1].set_title('Accuracy trend')
plt.show()

from tensorflow.keras.metrics import Precision,Recall,BinaryAccuracy

pre=Precision()
rec=Recall()
acc=BinaryAccuracy()

for batch in val_data.as_numpy_iterator():
    reviews,sentiments=batch
    preds=model.predict(reviews,verbose=0)
    sentiments=sentiments.flatten()
    preds=preds.flatten()
    
    pre.update_state(sentiments,preds)
    rec.update_state(sentiments,preds)
    acc.update_state(sentiments,preds)

print(f'Precision: {pre.result()}')
print(f'Recall: {rec.result()}')
print(f'Binary Accuracy: {acc.result()}')

model.save('/content/gdrive/MyDrive/data/metadata')
tf.keras.models.save_model(model,'/content/gdrive/MyDrive/data/sentiment_classifier.h5')

model=tf.keras.models.load_model('/content/gdrive/MyDrive/data/sentiment_classifier.h5',compile=False)
# model.load_weights('ckpt')
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy']
)

model.evaluate(val_data)

def build_inference_model(model):
    inputs=Input(shape=(1,),dtype='string')
    ids=vectorize_layer(inputs)
    outputs=model(ids)
    model=Model(inputs=inputs,outputs=outputs)
    return model

end2end_model=build_inference_model(model)
end2end_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]
)
end2end_model.summary()
end2end_model.save('/content/gdrive/MyDrive/data/end2end_model')

def get_sentiment(reviews):
    preds=end2end_model(np.array(reviews))
    preds=tf.squeeze(preds,axis=-1)
    temp=[]
    for pred in preds:
        temp.append({'Sentiment':'Positive','Confidence':pred.numpy()*100} if pred>.5 else ({'Sentiment':'Negative','Confidence':(1-pred).numpy()*100}))
    return temp

get_sentiment([
    'This is an absolutely outstanding and beautiful movie, and there wasn’t a single moment or scene in which I lost interest. I am not a movie person to say the least, but after hearing so much good about it from others I decided to check it out. It keeps you hooked and the storyline is something so timeless and unique! What really resonated with me during my first time watching this film was the main character, Ishida’s, personality. Having a character in a movie who lives with anxiety and feeling truly alone is something I could most definitely relate to. ',
    
    '''It’s just horrible.... the mc can’t defend herself from this assault! Like she doesn’t have any grudges or anything is like she forgets the insults and harassment they did.... she is not normal! She always talking to them like they are buddies and why when they ask yui a question she says “well” or “umm” LIKE SAY SOMETHING she barely said anything in most of the episodes of season 1''',
    
    '''Don’t listen to the 0 stars people this anime is beautiful and a great experience (still can’t believe boss baby beat this in the oscars) people saying it’s too long honestly make zero sense. Endgame is 3 hours now is it considered a bad movie because of it, HELL NO it’s considered one of the greatest superhero movies ever made.''',
    '''I'm so done with vampires... and Yui. Why is she such a weakling? Honestly her character frustrates me a lot. I watched this show quite some time ago and honestly I don't know why I did, probably because I was a kid and didn't really understand. honestly I would not recommend this anime :/''',
    
    '''Highly Overrated But Only Good 

However delightful as it is The Shawshank Redemption has no allegories and no statements to make and it brought nothing new to cinema in either style or substance. 
It had no revolutionary ideas or approaches it's only a very safe buddy story A damn good one but still just that. So it can impossibly be a film that will be remembered or studied, because what will it be remembered for Friendship Huh neat At the end of the day Shawshank is a good film but far from a perfect one. '''
])